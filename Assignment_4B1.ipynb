{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_4B1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4ZykrQIQCUu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "81c22e78-2881-4098-c0e7-ecd93d069ab5"
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import os\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF1RyWNeSa6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training parameters\n",
        "batch_size = 128  # orig paper trained all networks with batch_size=128\n",
        "epochs = 50\n",
        "data_augmentation = True\n",
        "num_classes = 10\n",
        "\n",
        "# Subtracting pixel mean improves accuracy\n",
        "subtract_pixel_mean = True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuHGiR6RShGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 3\n",
        "\n",
        "# Model version\n",
        "# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
        "version = 1\n",
        "\n",
        "# Computed depth from supplied model parameter n\n",
        "if version == 1:\n",
        "    depth = n * 6 + 2\n",
        "elif version == 2:\n",
        "    depth = n * 9 + 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXG2wCB7SuBF",
        "colab_type": "code",
        "outputId": "e9c43544-9aee-4be7-a8ba-6ab03620d53c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model_type = 'ResNet%dv%d' % (depth, version)\n",
        "print(model_type)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet20v1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRNlJhTeSydp",
        "colab_type": "code",
        "outputId": "91ab6e9f-c775-4234-e1c7-175a8193c20a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Load the CIFAR10 data.\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Input image dimensions.\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# Normalize data.\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "print(input_shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n",
            "(32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkfcoIIRS3t7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If subtract pixel mean is enabled\n",
        "if subtract_pixel_mean:\n",
        "    x_train_mean = np.mean(x_train, axis=0)\n",
        "    x_train -= x_train_mean\n",
        "    x_test -= x_train_mean\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbahf-zoS8Sm",
        "colab_type": "code",
        "outputId": "b9138b55-6d4d-4ab4-8a9e-8c20b7d17951",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print('y_train shape:', y_train.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "y_train shape: (50000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bliy7uQCS_TV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztYG7WbcTD38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 160:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN8XDVqBTLA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfKWSTr8TPnn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_v1(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "    ResNet32 0.46M\n",
        "    ResNet44 0.66M\n",
        "    ResNet56 0.85M\n",
        "    ResNet110 1.7M\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIEvlYf2TWlB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_v2(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 2 Model builder [b]\n",
        "\n",
        "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
        "    bottleneck layer\n",
        "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
        "    Second and onwards shortcut connection is identity.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filter maps is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same filter map sizes.\n",
        "    Features maps sizes:\n",
        "    conv1  : 32x32,  16\n",
        "    stage 0: 32x32,  64\n",
        "    stage 1: 16x16, 128\n",
        "    stage 2:  8x8,  256\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNUrkGIoTeyC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "outputId": "a079c933-37d6-482d-904b-3ee08fa38e50"
      },
      "source": [
        "if version == 2:\n",
        "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
        "else:\n",
        "    model = resnet_v1(input_shape=input_shape, depth=depth)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2xOYw1XoB_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_model_history(model_history):\n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
        "    # summarize history for accuracy\n",
        "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
        "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
        "    axs[0].legend(['train', 'val'], loc='best')\n",
        "    # summarize history for loss\n",
        "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
        "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
        "    axs[1].legend(['train', 'val'], loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_bNwBZuvxHx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "00b5a655-d4a8-48dd-f6a2-f640a0575c23"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),  #lr_schedule(100) #lr=lr_schedule(50)\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njPe_s3baL6U",
        "colab_type": "code",
        "outputId": "39ddf59d-5ae2-48dd-9e2f-ecadee236263",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 16)   0           activation_1[0][0]               \n",
            "                                                                 batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 16)   0           activation_3[0][0]               \n",
            "                                                                 batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 32, 32, 16)   0           activation_5[0][0]               \n",
            "                                                                 batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4640        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 32)   544         activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 16, 16, 32)   0           conv2d_10[0][0]                  \n",
            "                                                                 batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 32)   0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 16, 16, 32)   0           activation_9[0][0]               \n",
            "                                                                 batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 16, 16, 32)   0           activation_11[0][0]              \n",
            "                                                                 batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 32)   0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 8, 8, 64)     18496       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 8, 8, 64)     2112        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 8, 8, 64)     0           conv2d_17[0][0]                  \n",
            "                                                                 batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 8, 8, 64)     0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 8, 8, 64)     0           activation_15[0][0]              \n",
            "                                                                 batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 8, 8, 64)     0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 8, 8, 64)     36928       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 8, 8, 64)     0           activation_17[0][0]              \n",
            "                                                                 batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 8, 8, 64)     0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 274,442\n",
            "Trainable params: 273,066\n",
            "Non-trainable params: 1,376\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUfOMEAVaMxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "#lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer] #, lr_scheduler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7eivnrDwbtt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdwG-0ddwpkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pixel_level = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXsvKsnwvkOo",
        "colab_type": "code",
        "outputId": "2155fe2e-406f-4a2f-9278-d02d7617f39f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Run training, with or without data augmentation.\n",
        "#from random_eraser import get_random_eraser\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True\n",
        "              ) #callbacks=callbacks\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False,  # randomly flip images\n",
        "        preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=pixel_level))\n",
        "    # Compute quantities required for featurewise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    model_info =  model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        epochs=epochs, verbose=1, workers=4,\n",
        "                        callbacks=callbacks) "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/50\n",
            "391/391 [==============================] - 52s 133ms/step - loss: 1.7821 - acc: 0.4150 - val_loss: 1.7228 - val_acc: 0.4301\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.43010, saving model to /content/saved_models/cifar10_ResNet20v1_model.001.h5\n",
            "Epoch 2/50\n",
            "391/391 [==============================] - 43s 110ms/step - loss: 1.4003 - acc: 0.5489 - val_loss: 1.6469 - val_acc: 0.4854\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.43010 to 0.48540, saving model to /content/saved_models/cifar10_ResNet20v1_model.002.h5\n",
            "Epoch 3/50\n",
            "391/391 [==============================] - 43s 111ms/step - loss: 1.2224 - acc: 0.6167 - val_loss: 1.2772 - val_acc: 0.6019\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.48540 to 0.60190, saving model to /content/saved_models/cifar10_ResNet20v1_model.003.h5\n",
            "Epoch 4/50\n",
            "391/391 [==============================] - 43s 109ms/step - loss: 1.1024 - acc: 0.6591 - val_loss: 1.1941 - val_acc: 0.6442\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.60190 to 0.64420, saving model to /content/saved_models/cifar10_ResNet20v1_model.004.h5\n",
            "Epoch 5/50\n",
            "391/391 [==============================] - 42s 108ms/step - loss: 1.0224 - acc: 0.6898 - val_loss: 0.9835 - val_acc: 0.7127\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.64420 to 0.71270, saving model to /content/saved_models/cifar10_ResNet20v1_model.005.h5\n",
            "Epoch 6/50\n",
            "391/391 [==============================] - 42s 107ms/step - loss: 0.9611 - acc: 0.7124 - val_loss: 1.6867 - val_acc: 0.5889\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.71270\n",
            "Epoch 7/50\n",
            "391/391 [==============================] - 43s 109ms/step - loss: 0.9084 - acc: 0.7332 - val_loss: 0.9593 - val_acc: 0.7258\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.71270 to 0.72580, saving model to /content/saved_models/cifar10_ResNet20v1_model.007.h5\n",
            "Epoch 8/50\n",
            "391/391 [==============================] - 42s 107ms/step - loss: 0.8717 - acc: 0.7463 - val_loss: 1.0031 - val_acc: 0.7155\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.72580\n",
            "Epoch 9/50\n",
            "391/391 [==============================] - 42s 107ms/step - loss: 0.8370 - acc: 0.7571 - val_loss: 1.1895 - val_acc: 0.6467\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.72580\n",
            "Epoch 10/50\n",
            "391/391 [==============================] - 42s 107ms/step - loss: 0.8112 - acc: 0.7669 - val_loss: 1.1418 - val_acc: 0.6985\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.72580\n",
            "Epoch 11/50\n",
            "391/391 [==============================] - 42s 107ms/step - loss: 0.7814 - acc: 0.7782 - val_loss: 1.0857 - val_acc: 0.7069\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.72580\n",
            "Epoch 12/50\n",
            "391/391 [==============================] - 42s 108ms/step - loss: 0.7642 - acc: 0.7806 - val_loss: 0.8353 - val_acc: 0.7699\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.72580 to 0.76990, saving model to /content/saved_models/cifar10_ResNet20v1_model.012.h5\n",
            "Epoch 13/50\n",
            "391/391 [==============================] - 41s 106ms/step - loss: 0.7423 - acc: 0.7925 - val_loss: 0.8419 - val_acc: 0.7669\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.76990\n",
            "Epoch 14/50\n",
            "391/391 [==============================] - 42s 107ms/step - loss: 0.7265 - acc: 0.7971 - val_loss: 0.8595 - val_acc: 0.7681\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.76990\n",
            "Epoch 15/50\n",
            "391/391 [==============================] - 42s 106ms/step - loss: 0.7108 - acc: 0.8024 - val_loss: 0.7950 - val_acc: 0.7868\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.76990 to 0.78680, saving model to /content/saved_models/cifar10_ResNet20v1_model.015.h5\n",
            "Epoch 16/50\n",
            "391/391 [==============================] - 41s 106ms/step - loss: 0.6972 - acc: 0.8059 - val_loss: 0.8341 - val_acc: 0.7674\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.78680\n",
            "Epoch 17/50\n",
            "391/391 [==============================] - 41s 105ms/step - loss: 0.6838 - acc: 0.8118 - val_loss: 0.9007 - val_acc: 0.7631\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.78680\n",
            "Epoch 18/50\n",
            "391/391 [==============================] - 41s 105ms/step - loss: 0.6723 - acc: 0.8153 - val_loss: 0.9605 - val_acc: 0.7498\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.78680\n",
            "Epoch 19/50\n",
            "391/391 [==============================] - 42s 108ms/step - loss: 0.6557 - acc: 0.8237 - val_loss: 0.8085 - val_acc: 0.7827\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.78680\n",
            "Epoch 20/50\n",
            "391/391 [==============================] - 42s 108ms/step - loss: 0.6505 - acc: 0.8235 - val_loss: 0.9267 - val_acc: 0.7502\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.78680\n",
            "Epoch 21/50\n",
            "391/391 [==============================] - 43s 111ms/step - loss: 0.5697 - acc: 0.8527 - val_loss: 0.5731 - val_acc: 0.8540\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.78680 to 0.85400, saving model to /content/saved_models/cifar10_ResNet20v1_model.021.h5\n",
            "Epoch 22/50\n",
            "391/391 [==============================] - 44s 112ms/step - loss: 0.5514 - acc: 0.8578 - val_loss: 0.5618 - val_acc: 0.8605\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.85400 to 0.86050, saving model to /content/saved_models/cifar10_ResNet20v1_model.022.h5\n",
            "Epoch 23/50\n",
            "391/391 [==============================] - 44s 113ms/step - loss: 0.5411 - acc: 0.8597 - val_loss: 0.5973 - val_acc: 0.8488\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.86050\n",
            "Epoch 24/50\n",
            "391/391 [==============================] - 46s 118ms/step - loss: 0.5286 - acc: 0.8641 - val_loss: 0.6042 - val_acc: 0.8448\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.86050\n",
            "Epoch 25/50\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.5222 - acc: 0.8645 - val_loss: 0.5456 - val_acc: 0.8642\n",
            "\n",
            "Epoch 00025: val_acc improved from 0.86050 to 0.86420, saving model to /content/saved_models/cifar10_ResNet20v1_model.025.h5\n",
            "Epoch 26/50\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.5175 - acc: 0.8666 - val_loss: 0.5445 - val_acc: 0.8646\n",
            "\n",
            "Epoch 00026: val_acc improved from 0.86420 to 0.86460, saving model to /content/saved_models/cifar10_ResNet20v1_model.026.h5\n",
            "Epoch 27/50\n",
            "391/391 [==============================] - 43s 111ms/step - loss: 0.5018 - acc: 0.8716 - val_loss: 0.5648 - val_acc: 0.8550\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.86460\n",
            "Epoch 28/50\n",
            "391/391 [==============================] - 43s 109ms/step - loss: 0.5020 - acc: 0.8711 - val_loss: 0.6485 - val_acc: 0.8315\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.86460\n",
            "Epoch 29/50\n",
            "391/391 [==============================] - 43s 109ms/step - loss: 0.4960 - acc: 0.8717 - val_loss: 0.5980 - val_acc: 0.8442\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.86460\n",
            "Epoch 30/50\n",
            "391/391 [==============================] - 42s 107ms/step - loss: 0.4893 - acc: 0.8737 - val_loss: 0.5299 - val_acc: 0.8672\n",
            "\n",
            "Epoch 00030: val_acc improved from 0.86460 to 0.86720, saving model to /content/saved_models/cifar10_ResNet20v1_model.030.h5\n",
            "Epoch 31/50\n",
            "391/391 [==============================] - 43s 109ms/step - loss: 0.4851 - acc: 0.8772 - val_loss: 0.5657 - val_acc: 0.8582\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.86720\n",
            "Epoch 32/50\n",
            "391/391 [==============================] - 43s 111ms/step - loss: 0.4795 - acc: 0.8773 - val_loss: 0.5416 - val_acc: 0.8653\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.86720\n",
            "Epoch 33/50\n",
            "391/391 [==============================] - 44s 111ms/step - loss: 0.4759 - acc: 0.8789 - val_loss: 0.5672 - val_acc: 0.8529\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.86720\n",
            "Epoch 34/50\n",
            "391/391 [==============================] - 44s 113ms/step - loss: 0.4706 - acc: 0.8804 - val_loss: 0.5841 - val_acc: 0.8512\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.86720\n",
            "Epoch 35/50\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.4631 - acc: 0.8832 - val_loss: 0.5926 - val_acc: 0.8548\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.86720\n",
            "Epoch 36/50\n",
            "391/391 [==============================] - 45s 115ms/step - loss: 0.4338 - acc: 0.8929 - val_loss: 0.5254 - val_acc: 0.8697\n",
            "\n",
            "Epoch 00036: val_acc improved from 0.86720 to 0.86970, saving model to /content/saved_models/cifar10_ResNet20v1_model.036.h5\n",
            "Epoch 37/50\n",
            "391/391 [==============================] - 44s 113ms/step - loss: 0.4316 - acc: 0.8938 - val_loss: 0.4846 - val_acc: 0.8813\n",
            "\n",
            "Epoch 00037: val_acc improved from 0.86970 to 0.88130, saving model to /content/saved_models/cifar10_ResNet20v1_model.037.h5\n",
            "Epoch 38/50\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.4220 - acc: 0.8954 - val_loss: 0.5119 - val_acc: 0.8733\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.88130\n",
            "Epoch 39/50\n",
            "391/391 [==============================] - 44s 114ms/step - loss: 0.4179 - acc: 0.8983 - val_loss: 0.4895 - val_acc: 0.8803\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.88130\n",
            "Epoch 40/50\n",
            "391/391 [==============================] - 44s 113ms/step - loss: 0.4184 - acc: 0.8962 - val_loss: 0.4802 - val_acc: 0.8841\n",
            "\n",
            "Epoch 00040: val_acc improved from 0.88130 to 0.88410, saving model to /content/saved_models/cifar10_ResNet20v1_model.040.h5\n",
            "Epoch 41/50\n",
            "391/391 [==============================] - 45s 115ms/step - loss: 0.4133 - acc: 0.8975 - val_loss: 0.4978 - val_acc: 0.8786\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.88410\n",
            "Epoch 42/50\n",
            "391/391 [==============================] - 44s 113ms/step - loss: 0.4132 - acc: 0.8990 - val_loss: 0.4967 - val_acc: 0.8777\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.88410\n",
            "Epoch 43/50\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.4085 - acc: 0.8993 - val_loss: 0.4848 - val_acc: 0.8821\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.88410\n",
            "Epoch 44/50\n",
            "391/391 [==============================] - 44s 113ms/step - loss: 0.4040 - acc: 0.9011 - val_loss: 0.4932 - val_acc: 0.8790\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.88410\n",
            "Epoch 45/50\n",
            "391/391 [==============================] - 45s 115ms/step - loss: 0.3998 - acc: 0.9022 - val_loss: 0.4828 - val_acc: 0.8808\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.88410\n",
            "Epoch 46/50\n",
            "391/391 [==============================] - 45s 115ms/step - loss: 0.3904 - acc: 0.9060 - val_loss: 0.4848 - val_acc: 0.8806\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.88410\n",
            "Epoch 47/50\n",
            "391/391 [==============================] - 45s 115ms/step - loss: 0.3876 - acc: 0.9061 - val_loss: 0.4727 - val_acc: 0.8863\n",
            "\n",
            "Epoch 00047: val_acc improved from 0.88410 to 0.88630, saving model to /content/saved_models/cifar10_ResNet20v1_model.047.h5\n",
            "Epoch 48/50\n",
            "391/391 [==============================] - 44s 114ms/step - loss: 0.3864 - acc: 0.9072 - val_loss: 0.4803 - val_acc: 0.8839\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.88630\n",
            "Epoch 49/50\n",
            "391/391 [==============================] - 44s 114ms/step - loss: 0.3866 - acc: 0.9082 - val_loss: 0.4733 - val_acc: 0.8847\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.88630\n",
            "Epoch 50/50\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.3844 - acc: 0.9072 - val_loss: 0.4673 - val_acc: 0.8851\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.88630\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64C9xDvix0bz",
        "colab_type": "code",
        "outputId": "763883f7-8f1c-4d3f-af80-a70c9a632e3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 2s 204us/step\n",
            "Test loss: 0.4673044873714447\n",
            "Test accuracy: 0.8851\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkJrRQpRx41f",
        "colab_type": "code",
        "outputId": "40ac2e78-85d0-4645-bfca-8d4c66e9a89a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "plot_model_history(model_info)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAFNCAYAAABSRs15AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1BP/BZ2ve1Ay9lpQAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}